{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b951a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beeb0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 - 토큰화 & 정수 인코딩 - 시퀀스 패딩\n",
    "# baseline 일반 신경망 Dense\n",
    "# SimpleRNN  기울기 소실문제\n",
    "# Bidirecitional LSTM :  양방향 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3bd6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization  텍스트를 수치로 변환\n",
    "# Word Embedding  단어의 의미를 벡터로 표현\n",
    "# Sequence Padding  시퀀스 길이 맞추기\n",
    "# SimpleRNN  순환 신경망\n",
    "# LSTM      RNN의 경사소실문제 해결\n",
    "# Bidirectional LSTM  양방향 컨텐츠 학습\n",
    "# Binary Classification  이진 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb1ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 : 숫자로 변환하는 과정\n",
    "# 토크나이저 3단계\n",
    "    # 1. fit_on_texts()  가장 빈도가 높은 단어의 인덱스를 구축해서 딕셔너리\n",
    "    # 2. texts_to_sequences()  텍스트를 정수 인덱스 시퀀스로 변환\n",
    "    # 3. pad_sequences()  시퀀스 길이 맞추기 (패딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a0a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝에서 워드 임배딩 레이어 : 각 단어를 고정된 크기의 실수 벡터\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c21ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [\n",
    "    \"this movie is great and wonderful\",\n",
    "    \"bad movie with poor acting\",\n",
    "    \"great movie absolutely wonderful\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7963afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 버전\n",
    "# 토큰화\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8802e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 단어 분할 및 빈도 계산\n",
    "all_words = []\n",
    "for i in [review.split() for review in sample_reviews]:\n",
    "    all_words.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d21cf2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도\n",
    "word_freq = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54f6c864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 인덱스: {'UNK': 1, 'movie': 2, 'great': 3, 'wonderful': 4, 'this': 5, 'is': 6, 'and': 7, 'bad': 8, 'with': 9, 'poor': 10}\n",
      "정수 시퀀스: [[5, 2, 6, 3, 7, 4], [8, 2, 9, 10, 1], [3, 2, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "# 2. Tokkenizer 구현\n",
    "class  SimpleTokenizer:\n",
    "    def __init__(self, num_words=10, oov_token=\"UNK\"):\n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        all_words = []\n",
    "        for i in [review.split() for review in texts]:\n",
    "            all_words.extend(i)\n",
    "        word_freq = Counter(all_words)\n",
    "        # 빈도 높은 순서로 인덱스 부여\n",
    "        # oov 토큰을 1로 설정\n",
    "        self.word_index[self.oov_token] = 1\n",
    "        self.index_word[1] = self.oov_token\n",
    "        idx = 2\n",
    "        for word, freq in word_freq.most_common(self.num_words - 1):\n",
    "            self.word_index[word] = idx\n",
    "            self.index_word[idx] = word\n",
    "            idx += 1\n",
    "    def texts_to_sequences(self, texts):\n",
    "        '''텍스트를 정수 시퀀스로 변환'''\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            seq = []\n",
    "            for word in text.split():\n",
    "                # 단어가 vocalulary에 있으면 인덱스를 사용 없으면 oov\n",
    "                word_index = self.word_index.get(word, 1)\n",
    "                seq.append(word_index)\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "# Tokenizer 생성 및 학습\n",
    "tokenizer = SimpleTokenizer(num_words=10, oov_token=\"UNK\")\n",
    "tokenizer.fit_on_texts(sample_reviews)\n",
    "print(\"단어 인덱스:\", tokenizer.word_index)\n",
    "# 텍스트를 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences(sample_reviews)\n",
    "print(\"정수 시퀀스:\", sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ee65901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩된 시퀀스:\n",
      " [[ 0  0  0  0  5  2  6  3  7  4]\n",
      " [ 0  0  0  0  0  8  2  9 10  1]\n",
      " [ 0  0  0  0  0  0  3  2  1  4]]\n"
     ]
    }
   ],
   "source": [
    "# 패딩 구현 - 문자열의 길이를 동일하게 맞춘다\n",
    "def pad_sequence_manual(squence, max_len=10, padding='pre', value=0):\n",
    "    '''패딩구현'''\n",
    "    padded = []\n",
    "    for seq in squence:\n",
    "        if len(seq) >= max_len:\n",
    "            if padding == 'pre':\n",
    "                padded_seq = seq[-max_len:]\n",
    "            else:\n",
    "                padded_seq = seq[:max_len]\n",
    "        else:\n",
    "            pad_length = max_len - len(seq)\n",
    "            if padding == 'pre':\n",
    "                padded_seq = [value] * pad_length + seq\n",
    "            else:\n",
    "                padded_seq = seq + [value] * pad_length\n",
    "        padded.append(padded_seq)\n",
    "    return np.array(padded)\n",
    "# 패딩 적용\n",
    "padded_sequences = pad_sequence_manual(sequences, max_len=10, padding='pre', value=0)\n",
    "print(\"패딩된 시퀀스:\\n\", padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0bb50bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  5,  2,  6,  3,  7,  4],\n",
       "        [ 0,  0,  0,  0,  0,  8,  2,  9, 10,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  3,  2,  1,  4]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyTorch 텐서로 변환\n",
    "sequence_tensor = torch.LongTensor(padded_sequences)\n",
    "sequence_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab6567f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩된 시퀀스 형태 : (3, 10)\n",
      "첫번째 : tensor([0, 0, 0, 0, 5, 2, 6, 3, 7, 4])\n",
      "입력형태 : torch.Size([3, 10])\n",
      "출력형태 : torch.Size([3, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "# 2. 워드 임배딩 - 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch.nn as nn\n",
    "print(f'패딩된 시퀀스 형태 : {padded_sequences.shape}')\n",
    "print(f'첫번째 : {sequence_tensor[0]}')\n",
    "# pythorch emvedding 레이어 생성\n",
    "# num_embeddings 어휘의 크기\n",
    "# embedding_dim 각 단어를 몇차원 벡터로 표현할 것인지\n",
    "emvbedding_layer = nn.Embedding(num_embeddings=1000, embedding_dim=8, padding_idx=0)\n",
    "embedded = emvbedding_layer(sequence_tensor)\n",
    "print(f'입력형태 : {sequence_tensor.shape}')\n",
    "print(f'출력형태 : {embedded.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79b37225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 ID: 0, 임베딩 벡터: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "단어 ID: 0, 임베딩 벡터: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "단어 ID: 0, 임베딩 벡터: [0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 벡터 상세 분석\n",
    "# 데이터 3개\n",
    "for word_idx in range(3):\n",
    "    embedding_vec = embedded[0, word_idx].detach().numpy()\n",
    "    word_id = sequence_tensor[0, word_idx].item()\n",
    "    print(f'단어 ID: {word_id}, 임베딩 벡터: {embedding_vec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e1b8ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입베딩 행렬 형태 : (1000, 8)\n",
      "패딩 id = 0의 임베딩 벡터 : [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "단어 id = 5인 임베딩 벡터 : [ 0.49397928 -0.5430416   0.9405944  -1.8554078   0.45127866  0.9239473\n",
      " -0.33081326  1.3918082 ]\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 행렬\n",
    "emvbedding_matrix = emvbedding_layer.weight.detach().numpy()\n",
    "print(f'입베딩 행렬 형태 : {emvbedding_matrix.shape}')\n",
    "print(f'패딩 id = 0의 임베딩 벡터 : {emvbedding_matrix[0]}')\n",
    "print(f'단어 id = 5인 임베딩 벡터 : {emvbedding_matrix[5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9105e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7bed77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 적용\n",
    "class RnnModule(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RnnModule, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        last_hidden_state = rnn_out[:, -1, :]\n",
    "        out = self.fc(last_hidden_state)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
